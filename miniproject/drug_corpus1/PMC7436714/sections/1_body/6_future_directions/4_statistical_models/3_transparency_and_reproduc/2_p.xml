<?xml version="1.0" encoding="UTF-8"?>
<p>We strongly advocate for the urgent need to build models that are transparent and reproducible (Peng, 
 <xref rid="insr12402-bib-0071" ref-type="ref">2011</xref>). As most methods and models for the COVID‐19 pandemic are fairly recent and many have not yet been carefully peer reviewed, researchers should document the sources of data used, data preprocessing protocols, source computing code and sufficient modelling details to allow external validation from the public. Such details are also necessary to allow others, who may have better quality data but without sufficient statistical expertise, to easily adopt new methodologies to obtain high‐quality results. As mentioned in an original post by Dr Nilanjan Chatterjee (
 <ext-link ext-link-type="uri" xlink:href="https://link.medium.com/hqUQILEAd6" xmlns:xlink="http://www.w3.org/1999/xlink">https://link.medium.com/hqUQILEAd6</ext-link>), transparency, reproducibility and validity are three criteria to assess and assure the quality of prediction models. His essay also mentioned the difficulty in reproducing the work given by the IHME to obtain accurate predictions and appropriate confidence intervals. Similar to the IHME method that has no software available, Gu's method for the COVID‐19 prediction (
 <ext-link ext-link-type="uri" xlink:href="https://covid19-projections.com/" xmlns:xlink="http://www.w3.org/1999/xlink">https://covid19‐projections.com/</ext-link>) that has recently received much attention does not provide software, either, unfortunately. Without clear guidance and full reproducibility, even models that currently do well might fail in the future because predictions are relying on certain kinds of extrapolation assumptions that need to be unveiled to the scientific community with full transparency for validation and comparison.
</p>
