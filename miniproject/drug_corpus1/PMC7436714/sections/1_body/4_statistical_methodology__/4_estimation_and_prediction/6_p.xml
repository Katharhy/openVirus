<?xml version="1.0" encoding="UTF-8"?>
<p>MCMC procedures are well suited for the estimation and inference in the setting of state‐space models because of fast and reliable numerical performances. For the Michigan data analysis example in Section 
 <xref rid="insr12402-sec-0031" ref-type="sec">5.5</xref>, using an average personal computer, we spend 1.5 h completing all MCMC calculations of 200 000 draws with thinning bin size of 10 after the burn‐in judged by four separate MCMC chains. This computing speed can be improved by using high‐performance computing facilities and/or some recent posterior sampling methods. As suggested by Zhou and Ji (
 <xref rid="insr12402-bib-0101" ref-type="ref">2020</xref>) for a state‐space SIR model, one may set a more efficient sampler over highly correlated posterior spaces by parallel‐tempering MCMC algorithm (Geyer, 
 <xref rid="insr12402-bib-0041" ref-type="ref">1991</xref>), which provides rapid mixing in MCMC chains. Also, along the line of online learning, sequential Monte Carlo methods for posterior sampling (Doucet 
 <italic>et al.</italic>, 
 <xref rid="insr12402-bib-0031" ref-type="ref">2001</xref>; Dukic 
 <italic>et al.</italic>, 
 <xref rid="insr12402-bib-0032" ref-type="ref">2012</xref>) are promising, as they permit efficient updating of existing posteriors with sequentially arrived data, in the hope to avoid refitting the model by running MCMC from scratch using the updated complete data.
</p>
