{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenVirus Jupyter Notebook\n",
    "\n",
    "### AMIDICT:  getpapers + dictionary creation \n",
    "\n",
    "NOTE: _All work is done in a single folder, hence paths to specific files needn't be specified_\n",
    "\n",
    "Ambreen Hamadani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GETPAPERS \n",
    "\"This downloads articles from EPMC; it may take some minutes if there are hundreds of files, especially large PDFs. Ignore the warnings about \"Not Open Access\", but you will end up with maybe 5% fewer papers than you asked for \"\n",
    "#### Using Get papers to create a communal copus\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of getpapers\n",
    "\n",
    "Requirements: NodeJS should be installed on your PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\nodejs\\getpapers -> C:\\Program Files\\nodejs\\node_modules\\getpapers\\bin\\getpapers.js\n",
      "C:\\Program Files\\nodejs\n",
      "`-- getpapers@0.4.17 \n",
      "  +-- chalk@1.0.0 \n",
      "  | +-- ansi-styles@2.2.1 \n",
      "  | +-- escape-string-regexp@1.0.5 \n",
      "  | +-- has-ansi@1.0.3 \n",
      "  | | +-- ansi-regex@1.1.1 \n",
      "  | | `-- get-stdin@4.0.1 \n",
      "  | +-- strip-ansi@2.0.1 \n",
      "  | `-- supports-color@1.3.1 \n",
      "  +-- commander@2.7.1 \n",
      "  | `-- graceful-readlink@1.0.1 \n",
      "  +-- crossref@0.1.2 \n",
      "  | +-- got@5.1.0 \n",
      "  | | +-- create-error-class@2.0.1 \n",
      "  | | | `-- capture-stack-trace@1.0.1 \n",
      "  | | +-- is-plain-obj@1.1.0 \n",
      "  | | +-- is-redirect@1.0.0 \n",
      "  | | +-- node-status-codes@1.0.0 \n",
      "  | | +-- object-assign@4.1.1 \n",
      "  | | +-- parse-json@2.2.0 \n",
      "  | | | `-- error-ex@1.3.2 \n",
      "  | | |   `-- is-arrayish@0.2.1 \n",
      "  | | +-- pinkie-promise@1.0.0 \n",
      "  | | | `-- pinkie@1.0.0 \n",
      "  | | +-- read-all-stream@3.1.0 \n",
      "  | | | `-- pinkie-promise@2.0.1 \n",
      "  | | |   `-- pinkie@2.0.4 \n",
      "  | | +-- unzip-response@1.0.2 \n",
      "  | | `-- url-parse-lax@1.0.0 \n",
      "  | `-- request@2.65.0 \n",
      "  |   +-- aws-sign2@0.6.0 \n",
      "  |   +-- bl@1.0.3 \n",
      "  |   | `-- readable-stream@2.0.6 \n",
      "  |   |   +-- process-nextick-args@1.0.7 \n",
      "  |   |   `-- string_decoder@0.10.31 \n",
      "  |   +-- caseless@0.11.0 \n",
      "  |   +-- combined-stream@1.0.8 \n",
      "  |   | `-- delayed-stream@1.0.0 \n",
      "  |   +-- forever-agent@0.6.1 \n",
      "  |   +-- form-data@1.0.1 \n",
      "  |   | `-- async@2.6.3 \n",
      "  |   |   `-- lodash@4.17.20 \n",
      "  |   +-- har-validator@2.0.6 \n",
      "  |   | +-- chalk@1.1.3 \n",
      "  |   | | +-- has-ansi@2.0.0 \n",
      "  |   | | | `-- ansi-regex@2.1.1 \n",
      "  |   | | +-- strip-ansi@3.0.1 \n",
      "  |   | | `-- supports-color@2.0.0 \n",
      "  |   | +-- commander@2.20.3 \n",
      "  |   | +-- is-my-json-valid@2.20.5 \n",
      "  |   | | +-- generate-function@2.3.1 \n",
      "  |   | | | `-- is-property@1.0.2 \n",
      "  |   | | +-- generate-object-property@1.2.0 \n",
      "  |   | | +-- is-my-ip-valid@1.0.0 \n",
      "  |   | | +-- jsonpointer@4.1.0 \n",
      "  |   | | `-- xtend@4.0.2 \n",
      "  |   | `-- pinkie-promise@2.0.1 \n",
      "  |   |   `-- pinkie@2.0.4 \n",
      "  |   +-- hawk@3.1.3 \n",
      "  |   | +-- boom@2.10.1 \n",
      "  |   | +-- cryptiles@2.0.5 \n",
      "  |   | +-- hoek@2.16.3 \n",
      "  |   | `-- sntp@1.0.9 \n",
      "  |   +-- http-signature@0.11.0 \n",
      "  |   | +-- asn1@0.1.11 \n",
      "  |   | +-- assert-plus@0.1.5 \n",
      "  |   | `-- ctype@0.5.3 \n",
      "  |   +-- json-stringify-safe@5.0.1 \n",
      "  |   +-- mime-types@2.1.27 \n",
      "  |   | `-- mime-db@1.44.0 \n",
      "  |   +-- node-uuid@1.4.8 \n",
      "  |   +-- oauth-sign@0.8.2 \n",
      "  |   +-- qs@5.2.1 \n",
      "  |   +-- stringstream@0.0.6 \n",
      "  |   +-- tough-cookie@2.2.2 \n",
      "  |   `-- tunnel-agent@0.4.3 \n",
      "  +-- got@2.9.2 \n",
      "  | +-- duplexify@3.7.1 \n",
      "  | | +-- end-of-stream@1.4.4 \n",
      "  | | +-- inherits@2.0.4 \n",
      "  | | +-- readable-stream@2.3.7 \n",
      "  | | | +-- core-util-is@1.0.2 \n",
      "  | | | +-- isarray@1.0.0 \n",
      "  | | | +-- process-nextick-args@2.0.1 \n",
      "  | | | +-- string_decoder@1.1.1 \n",
      "  | | | `-- util-deprecate@1.0.2 \n",
      "  | | `-- stream-shift@1.0.1 \n",
      "  | +-- infinity-agent@2.0.3 \n",
      "  | +-- is-stream@1.1.0 \n",
      "  | +-- lowercase-keys@1.0.1 \n",
      "  | +-- nested-error-stacks@1.0.2 \n",
      "  | +-- object-assign@2.1.1 \n",
      "  | +-- prepend-http@1.0.4 \n",
      "  | +-- read-all-stream@2.2.0 \n",
      "  | +-- statuses@1.5.0 \n",
      "  | `-- timed-out@2.0.0 \n",
      "  +-- lodash@3.10.1 \n",
      "  +-- matched@0.4.4 \n",
      "  | +-- arr-union@3.1.0 \n",
      "  | +-- async-array-reduce@0.2.1 \n",
      "  | +-- extend-shallow@2.0.1 \n",
      "  | | `-- is-extendable@0.1.1 \n",
      "  | +-- fs-exists-sync@0.1.0 \n",
      "  | +-- glob@7.1.6 \n",
      "  | | +-- fs.realpath@1.0.0 \n",
      "  | | +-- inflight@1.0.6 \n",
      "  | | | `-- wrappy@1.0.2 \n",
      "  | | +-- minimatch@3.0.4 \n",
      "  | | | `-- brace-expansion@1.1.11 \n",
      "  | | |   +-- balanced-match@1.0.0 \n",
      "  | | |   `-- concat-map@0.0.1 \n",
      "  | | +-- once@1.4.0 \n",
      "  | | `-- path-is-absolute@1.0.1 \n",
      "  | +-- has-glob@0.1.1 \n",
      "  | | `-- is-glob@2.0.1 \n",
      "  | |   `-- is-extglob@1.0.0 \n",
      "  | +-- is-valid-glob@0.3.0 \n",
      "  | +-- lazy-cache@2.0.2 \n",
      "  | | `-- set-getter@0.1.0 \n",
      "  | |   `-- to-object-path@0.3.0 \n",
      "  | |     `-- kind-of@3.2.2 \n",
      "  | |       `-- is-buffer@1.1.6 \n",
      "  | `-- resolve-dir@0.1.1 \n",
      "  |   +-- expand-tilde@1.2.2 \n",
      "  |   | `-- os-homedir@1.0.2 \n",
      "  |   `-- global-modules@0.2.3 \n",
      "  |     +-- global-prefix@0.1.5 \n",
      "  |     | +-- homedir-polyfill@1.0.3 \n",
      "  |     | | `-- parse-passwd@1.0.0 \n",
      "  |     | +-- ini@1.3.5 \n",
      "  |     | `-- which@1.3.1 \n",
      "  |     |   `-- isexe@2.0.0 \n",
      "  |     `-- is-windows@0.2.0 \n",
      "  +-- mkdirp@0.5.5 \n",
      "  | `-- minimist@1.2.5 \n",
      "  +-- progress@1.1.8 \n",
      "  +-- requestretry@1.13.0 \n",
      "  | +-- extend@3.0.2 \n",
      "  | +-- lodash@4.17.20 \n",
      "  | +-- request@2.88.2 \n",
      "  | | +-- aws-sign2@0.7.0 \n",
      "  | | +-- aws4@1.10.1 \n",
      "  | | +-- caseless@0.12.0 \n",
      "  | | +-- form-data@2.3.3 \n",
      "  | | | `-- asynckit@0.4.0 \n",
      "  | | +-- har-validator@5.1.5 \n",
      "  | | | +-- ajv@6.12.6 \n",
      "  | | | | +-- fast-deep-equal@3.1.3 \n",
      "  | | | | +-- fast-json-stable-stringify@2.1.0 \n",
      "  | | | | +-- json-schema-traverse@0.4.1 \n",
      "  | | | | `-- uri-js@4.4.0 \n",
      "  | | | `-- har-schema@2.0.0 \n",
      "  | | +-- http-signature@1.2.0 \n",
      "  | | | +-- assert-plus@1.0.0 \n",
      "  | | | +-- jsprim@1.4.1 \n",
      "  | | | | +-- assert-plus@1.0.0 \n",
      "  | | | | +-- extsprintf@1.3.0 \n",
      "  | | | | +-- json-schema@0.2.3 \n",
      "  | | | | `-- verror@1.10.0 \n",
      "  | | | |   `-- assert-plus@1.0.0 \n",
      "  | | | `-- sshpk@1.16.1 \n",
      "  | | |   +-- asn1@0.2.4 \n",
      "  | | |   +-- assert-plus@1.0.0 \n",
      "  | | |   +-- bcrypt-pbkdf@1.0.2 \n",
      "  | | |   +-- dashdash@1.14.1 \n",
      "  | | |   | `-- assert-plus@1.0.0 \n",
      "  | | |   +-- ecc-jsbn@0.1.2 \n",
      "  | | |   +-- getpass@0.1.7 \n",
      "  | | |   | `-- assert-plus@1.0.0 \n",
      "  | | |   +-- jsbn@0.1.1 \n",
      "  | | |   +-- safer-buffer@2.1.2 \n",
      "  | | |   `-- tweetnacl@0.14.5 \n",
      "  | | +-- is-typedarray@1.0.0 \n",
      "  | | +-- oauth-sign@0.9.0 \n",
      "  | | +-- performance-now@2.1.0 \n",
      "  | | +-- qs@6.5.2 \n",
      "  | | +-- safe-buffer@5.1.2 \n",
      "  | | +-- tough-cookie@2.5.0 \n",
      "  | | | +-- psl@1.8.0 \n",
      "  | | | `-- punycode@2.1.1 \n",
      "  | | +-- tunnel-agent@0.6.0 \n",
      "  | | `-- uuid@3.4.0 \n",
      "  | `-- when@3.7.8 \n",
      "  +-- restler@3.4.0 \n",
      "  | +-- iconv-lite@0.2.11 \n",
      "  | +-- qs@1.2.0 \n",
      "  | +-- xml2js@0.4.0 \n",
      "  | | +-- sax@0.5.8 \n",
      "  | | `-- xmlbuilder@15.1.1 \n",
      "  | `-- yaml@0.2.3 \n",
      "  +-- sanitize-filename@1.6.3 \n",
      "  | `-- truncate-utf8-bytes@1.0.2 \n",
      "  |   `-- utf8-byte-length@1.0.4 \n",
      "  +-- version_compare@0.0.3 \n",
      "  +-- winston@2.3.1 \n",
      "  | +-- async@1.0.0 \n",
      "  | +-- colors@1.0.3 \n",
      "  | +-- cycle@1.0.3 \n",
      "  | +-- eyes@0.1.8 \n",
      "  | +-- isstream@0.1.2 \n",
      "  | `-- stack-trace@0.0.10 \n",
      "  `-- xml2js@0.4.23 \n",
      "    +-- sax@1.2.4 \n",
      "    `-- xmlbuilder@11.0.1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "npm WARN deprecated request@2.65.0: request has been deprecated, see https://github.com/request/request/issues/3142\n",
      "npm WARN deprecated node-uuid@1.4.8: Use uuid module instead\n",
      "npm WARN deprecated tough-cookie@2.2.2: ReDoS vulnerability parsing Set-Cookie https://nodesecurity.io/advisories/130\n",
      "npm WARN deprecated hawk@3.1.3: This module moved to @hapi/hawk. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
      "npm WARN deprecated har-validator@2.0.6: this library is no longer supported\n",
      "npm WARN deprecated sntp@1.0.9: This module moved to @hapi/sntp. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
      "npm WARN deprecated boom@2.10.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
      "npm WARN deprecated cryptiles@2.0.5: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
      "npm WARN deprecated hoek@2.16.3: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
      "npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\n",
      "npm WARN deprecated har-validator@5.1.5: this library is no longer supported\n"
     ]
    }
   ],
   "source": [
    "!npm install --global getpapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the variables for the getpapers query \n",
    "HITS= 95\n",
    "QUERY= \"viral epidemic\"\n",
    "OUTDIR = \"v_epid\"\n",
    "LOGFILE= OUTDIR+ \"/\"+ \"log.txt\"\n",
    "FORMATS=\"-x -p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the functionality of the getpapers tool run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Usage: getpapers [options]\n",
      "\n",
      "  Options:\n",
      "\n",
      "    -h, --help                output usage information\n",
      "    -V, --version             output the version number\n",
      "    -q, --query <query>       search query (required)\n",
      "    -o, --outdir <path>       output directory (required - will be created if not found)\n",
      "    --api <name>              API to search [eupmc, crossref, ieee, arxiv] (default: eupmc)\n",
      "    -x, --xml                 download fulltext XMLs if available\n",
      "    -p, --pdf                 download fulltext PDFs if available\n",
      "    -s, --supp                download supplementary files if available\n",
      "    -t, --minedterms          download text-mined terms if available\n",
      "    -l, --loglevel <level>    amount of information to log (silent, verbose, info*, data, warn, error, or debug)\n",
      "    -a, --all                 search all papers, not just open access\n",
      "    -n, --noexecute           report how many results match the query, but don't actually download anything\n",
      "    -f, --logfile <filename>  save log to specified file in output directory as well as printing to terminal\n",
      "    -k, --limit <int>         limit the number of hits and downloads\n",
      "    --filter <filter object>  filter by key value pair, passed straight to the crossref api only\n",
      "    -r, --restart             restart file downloads after failure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!getpapers --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32minfo\u001b[39m: Saving logs to ./v_epid/log.txtÂ \n",
      "\u001b[32minfo\u001b[39m: Searching using eupmc API\n",
      "\u001b[32minfo\u001b[39m: Found 47963 open access results\n",
      "\u001b[33mwarn\u001b[39m: This version of getpapers wasn't built with this version of the EuPMC api in mind\n",
      "\u001b[33mwarn\u001b[39m: getpapers EuPMCVersion: 5.3.2 vs. 6.4 reported by api\n",
      "\u001b[32minfo\u001b[39m: Limiting to 95 hits\n",
      "\n",
      "\u001b[32minfo\u001b[39m: Done collecting results\n",
      "\u001b[32minfo\u001b[39m: limiting hits\n",
      "\u001b[32minfo\u001b[39m: Saving result metadata\n",
      "\u001b[32minfo\u001b[39m: Full EUPMC result metadata written to eupmc_results.json\n",
      "\u001b[32minfo\u001b[39m: Individual EUPMC result metadata records written\n",
      "\u001b[32minfo\u001b[39m: Extracting fulltext HTML URL list (may not be available for all articles)\n",
      "\u001b[32minfo\u001b[39m: Fulltext HTML URL list written to eupmc_fulltext_html_urls.txt\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.05.11.20098087 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.04.02.20048892 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7194687\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7571379\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7467093\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7554465\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7553173\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7543514\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7531915\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7558228\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7537317\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7566814\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7563921\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.08.29.20184473 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7510704\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7537651\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.07.20.20155390 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7543295\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7545019\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7518959\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.08.24.20180927 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.07.20.20157792 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with doi \"10.1101/2020.07.13.20152959 did not have a PMCID (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7543746\" was not Open Access (therefore no XML)\n",
      "\u001b[33mwarn\u001b[39m: Article with pmcid \"PMC7572080\" was not Open Access (therefore no XML)\n",
      "\u001b[32minfo\u001b[39m: Got XML URLs for 70 out of 95 results\n",
      "\u001b[32minfo\u001b[39m: Downloading fulltext XML files\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7462403 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7443314 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC6482703 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7505804 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7482922 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7548526 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7144339 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7527827 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7211730 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7503430 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7520648 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7265663 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7509824 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7442131 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7480627 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7491386 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7533885 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7313758 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7472829 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7523766 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7497550 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7241517 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7503432 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7427759 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7470771 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7239173 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7551987 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7560379 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7416915 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7412619 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7402651 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7461001 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7467064 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7535057 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7473198 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC6961731 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7318875 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7553064 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7414693 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7409795 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7506166 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7399656 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7448901 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7454403 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7546239 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7377713 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7545115 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7525083 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7460100 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7439087 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7396726 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7573891 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7461190 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7472723 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7394271 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7347327 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7498470 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7376323 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7472086 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7341469 already exists. Skipping.\n",
      "\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7300904 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7568032 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7547205 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7454349 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7500500 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7450302 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7409870 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7315692 already exists. Skipping.\n",
      "\u001b[32minfo\u001b[39m: File of type: XML and id: PMC7425942 already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "!getpapers -q \"$QUERY\" -o $OUTDIR -f $LOGFILE  $FORMATS -k $HITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionaries are made using WIKIDATA SPARQL \n",
    "The following code is about building the country dictionary first using SPARQL and then using amidict to validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install SPARQLWrapper \n",
    "from SPARQLWrapper import SPARQLWrapper, JSON,  XML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SPARL QUERY RESULT LOOKS LIKE THIS: \n",
      "*******************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikidata.value</th>\n",
       "      <th>wikidataLabel.value</th>\n",
       "      <th>_iso3166.value</th>\n",
       "      <th>wikipedia.value</th>\n",
       "      <th>alt.value</th>\n",
       "      <th>synonym.value</th>\n",
       "      <th>coords.value</th>\n",
       "      <th>term.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q16</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Canada</td>\n",
       "      <td>CA, ca, CDN, can, CAN, British North America, ...</td>\n",
       "      <td>CA, ca, CDN, can, CAN, British North America, ...</td>\n",
       "      <td>Point(-109.0 56.0)</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NL</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Netherlands</td>\n",
       "      <td>NL, Holland, Nederland, NED, nl, the Netherlan...</td>\n",
       "      <td>NL, Holland, Nederland, NED, nl, the Netherlan...</td>\n",
       "      <td>Point(5.55 52.316666666)</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q96</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>MX</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mexico</td>\n",
       "      <td>MEX, Mexican Republic, MX, mx, 🇲🇽, United Mexi...</td>\n",
       "      <td>MEX, Mexican Republic, MX, mx, United Mexican ...</td>\n",
       "      <td>Point(-102.0 23.0)</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q262</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Algeria</td>\n",
       "      <td>ALG, dz, 🇩🇿, People’s Democratic Republic of A...</td>\n",
       "      <td>ALG, dz, People’s Democratic Republic of Algeria</td>\n",
       "      <td>Point(1.0 28.0)</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q79</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>EG</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Egypt</td>\n",
       "      <td>EGY, eg, Republic of Egypt, Arab Rep. Egypt, A...</td>\n",
       "      <td>EGY, eg, Republic of Egypt, Arab Rep. Egypt, A...</td>\n",
       "      <td>Point(29.0 27.0)</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        wikidata.value wikidataLabel.value _iso3166.value  \\\n",
       "0   http://www.wikidata.org/entity/Q16              Canada             CA   \n",
       "1   http://www.wikidata.org/entity/Q55         Netherlands             NL   \n",
       "2   http://www.wikidata.org/entity/Q96              Mexico             MX   \n",
       "3  http://www.wikidata.org/entity/Q262             Algeria             DZ   \n",
       "4   http://www.wikidata.org/entity/Q79               Egypt             EG   \n",
       "\n",
       "                             wikipedia.value  \\\n",
       "0       https://en.wikipedia.org/wiki/Canada   \n",
       "1  https://en.wikipedia.org/wiki/Netherlands   \n",
       "2       https://en.wikipedia.org/wiki/Mexico   \n",
       "3      https://en.wikipedia.org/wiki/Algeria   \n",
       "4        https://en.wikipedia.org/wiki/Egypt   \n",
       "\n",
       "                                           alt.value  \\\n",
       "0  CA, ca, CDN, can, CAN, British North America, ...   \n",
       "1  NL, Holland, Nederland, NED, nl, the Netherlan...   \n",
       "2  MEX, Mexican Republic, MX, mx, 🇲🇽, United Mexi...   \n",
       "3  ALG, dz, 🇩🇿, People’s Democratic Republic of A...   \n",
       "4  EGY, eg, Republic of Egypt, Arab Rep. Egypt, A...   \n",
       "\n",
       "                                       synonym.value  \\\n",
       "0  CA, ca, CDN, can, CAN, British North America, ...   \n",
       "1  NL, Holland, Nederland, NED, nl, the Netherlan...   \n",
       "2  MEX, Mexican Republic, MX, mx, United Mexican ...   \n",
       "3   ALG, dz, People’s Democratic Republic of Algeria   \n",
       "4  EGY, eg, Republic of Egypt, Arab Rep. Egypt, A...   \n",
       "\n",
       "               coords.value   term.value  \n",
       "0        Point(-109.0 56.0)       Canada  \n",
       "1  Point(5.55 52.316666666)  Netherlands  \n",
       "2        Point(-102.0 23.0)       Mexico  \n",
       "3           Point(1.0 28.0)      Algeria  \n",
       "4          Point(29.0 27.0)        Egypt  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql.setQuery(\"\"\"\n",
    "## This query contains phrases common to all queries and others which are specific to particular dictionaries.(SPECIFIC) \n",
    "## Select Query was used to retrieve specific results (Country name, wiki data number, Synonyms) \n",
    "\n",
    "## ?code as ?_iso3166 is optional and is specific to countries \n",
    "\n",
    "SELECT ?wikidata ?wikidataLabel ?wikipedia (?wikidataAltLabel as ?alt) ?synonym (?wikidataLabel as ?term) ?wikidataDescription\n",
    "## (SPECIFIC) links to ?wikidata below\n",
    "    (?code as ?_iso3166) ?coords { \n",
    "\n",
    "## Forcing particular query execution order\n",
    "  hint:Query hint:optimizer \"None\" . \n",
    "\n",
    "## all ISO countries (SPECIFIC) must link to ?code above. \n",
    "  ?wikidata wdt:P297 ?code.\n",
    "  ?wikidata wdt:P625 ?coords.\n",
    "\n",
    "## Optional details about the terms like links to wikipaedia pages for each wikipedia page to be presented in a seperate column\n",
    "  OPTIONAL { ?wikipedia schema:about ?wikidata; schema:isPartOf <https://en.wikipedia.org/> }\n",
    "  SERVICE wikibase:label {\n",
    "    bd:serviceParam wikibase:language \"en\".\n",
    "\n",
    "## Selecting the prefered label \n",
    "    ?wikidata skos:altLabel ?wikidataAltLabel ; rdfs:label ?wikidataLabel; schema:description  ?wikidataDescription          \n",
    "  } \n",
    "\n",
    "## (SPECIFIC) Making sure the RGI alphabets of the flags are not rendered as flags and they appear as simple alphabets by specifying the acceptable characters. \n",
    "  BIND (REPLACE(REPLACE(?wikidataAltLabel, \"(, )?[🇦-🇿]{2}\", \"\"), \"^, \", \"\") AS ?synonym )\n",
    "      }\n",
    "\"\"\")\n",
    "\n",
    "sparql.setReturnFormat(XML)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "results1 = sparql.query().convert()\n",
    "\n",
    "print(\"THE SPARL QUERY RESULT LOOKS LIKE THIS: \")\n",
    "print(\"*******************************************\")\n",
    "\n",
    "results_df = pd.io.json.json_normalize(results1['results']['bindings'])\n",
    "results_df[['wikidata.value', 'wikidataLabel.value', '_iso3166.value', 'wikipedia.value', 'alt.value', 'synonym.value', 'coords.value' , 'term.value']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the sparql endpoint in country.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionary\\country.sparql.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(results.toxml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ami\n",
    "Requirements: JDK, Maven, git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/petermr/ami3.git\n",
    "!cd ami3\n",
    "!mvn install -Dmaven.test.skip=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Results should look like this "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[...]\n",
    "[INFO] Installing C:\\Users\\Desktop\\ami3\\pom.xml to C:\\Users\\.m2\\repository\\org\\contentmine\\ami3\\0.1-SNAPSHOT\\ami3-0.1-SNAPSHOT.pom\n",
    "[INFO] Installing C:\\Users\\Desktop\\ami3\\target\\ami3-0.1-SNAPSHOT-jar-with-dependencies.jar to C:\\Users\\.m2\\repository\\org\\contentmine\\ami3\\0.1-SNAPSHOT\\ami3-0.1-SNAPSHOT-jar-with-dependencies.jar\n",
    "[INFO] -------------------------------------------------------------\n",
    "[INFO] BUILD SUCCESS\n",
    "[INFO] -------------------------------------------------------------\n",
    "[INFO] Total time: 03:21 min\n",
    "[INFO] Finished at: 2020-06-13T36+05:30\n",
    "[INFO] -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMIDICT\n",
    "amidict converts the SPARQL output into the required dictionary format and also to validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: amidict [OPTIONS] COMMAND\n",
      "\n",
      "`amidict` is a command suite for managing dictionary:\n",
      "\n",
      "Parameters:\n",
      "===========\n",
      "      [@<filename>...]   One or more argument files containing options.\n",
      "Options:\n",
      "========\n",
      "  -d, --dictionary=<dictionaryNameList>[,<dictionaryNameList>...]...\n",
      "                         input or output dictionary NAMES/s. for 'create' must be singular; when 'display' or\n",
      "                           'translate', any number. Names should be lowercase, unique. [a-z][a-z0-9._]. Dots can be\n",
      "                           used to structure dictionaries intodirectories. Dictionary names are relative to\n",
      "                           'directory'. If <directory> is absent then dictionary names are absolute. ) This doesn't\n",
      "                           make sense; it should relate to current working directory.\n",
      "      --directory=<directory>\n",
      "                         top directory containing dictionary/s. Subdirectories will use structured names (NYI). Thus\n",
      "                           dictionary 'animals' is found in '<directory>/animals.xml', while 'plants.parts' is found in\n",
      "                           <directory>/plants/parts.xml. Required for relative dictionary names.\n",
      "  -h, --help             Show this help message and exit.\n",
      "  -V, --version          Print version information and exit.\n",
      "General Options:\n",
      "  -i, --input=FILE       Input filename, containing input for dictionary. its basename becomes the inputname\n",
      "  -n, --inputname=PATH   User's basename for inputfiles (e.g. foo/bar/<basename>.txt). The default name for the\n",
      "                           dictionary;  but may be obsolete and superseded by `dictionary`\n",
      "  -L, --inputnamelist=PATH...\n",
      "                         List of inputnames; will iterate over them, essentially compressing multiple commands into\n",
      "                           one. Experimental.\n",
      "  -x, --informat=PATH    extension for dictionary file , default: xml\n",
      "Logging Options:\n",
      "  -v, --verbose          Specify multiple -v options to increase verbosity. For example, `-v -v -v` or `-vvv`. We map\n",
      "                           ERROR or WARN -> 0 (i.e. always print), INFO -> 1(-v), DEBUG->2 (-vv)\n",
      "      --log4j=(CLASS LEVEL)...\n",
      "                         Customize logging configuration. Format: <classname> <level>; sets logging level of class, e.\n",
      "                           g.\n",
      "                          org.contentmine.ami.lookups.WikipediaDictionary INFO\n",
      "Commands:\n",
      "=========\n",
      "  create     creates dictionaries from text, Wikimedia, etc..\n",
      "  display    Displays AMI dictionaries. (Under Development)\n",
      "  search     searches within dictionaries\n",
      "  translate  translates dictionaries between formats\n",
      "  update     updates or merges dictionaries\n"
     ]
    }
   ],
   "source": [
    "!amidict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!amidict -vv --dictionary country --directory dictionary  --input dictionary/country.sparql.xml create --informat wikisparqlxml --sparqlmap name=wikidataLabel,term=wikidataLabel,description=wikidataDescription,wikidataURL=wikidata,wikidataID=wikidata,wikipediaPage=wikipedia,wikipediaURL=wikipedia,_p297_country=_iso3166,_coords=coords --transformName wikidataID=EXTRACT(wikidataURL,.*/(.*)) --synonyms=synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Results should look like this "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Version: amidict 2020.08.09_09.54-NEXT-SNAPSHOT\n",
    "(jar:file:/C:/Users/eless/ami3/target/appassembler/repo/ami3-2020.08.09_09.54-NEXT-SNAPSHOT.jar)\n",
    "JVM: 14.0.1 (Oracle Corporation Java HotSpot(TM) 64-Bit Server VM 14.0.1+7)\n",
    "OS: Windows 10 10.0 amd64\n",
    "\n",
    "\n",
    "Generic values (DictionaryCreationTool)\n",
    "================================\n",
    "--testString        : d      null\n",
    "--wikilinks         : d [Lorg.contentmine.ami.tools.AbstractAMIDictTool$WikiLink;@15dcfae7\n",
    "--datacols          : d      null\n",
    "--hrefcols          : d      null\n",
    "--informat          : m wikisparqlxml\n",
    "--linkcol           : d      null\n",
    "--namecol           : d      null\n",
    "--outformats        : d [Lorg.contentmine.ami.tools.AbstractAMIDictTool$DictionaryFileFormat;@17aad511\n",
    "--query             : d      null\n",
    "--sparqlmap         : m {name=wikidataLabel, term=wikidataLabel, description=wikidataDescription, wikidataURL=wikidata, wikidataID=wikidata, wikipediaPage=wikipedia, wikipediaURL=wikipedia, _p297_country=_iso3166}\n",
    "--sparqlquery       : d      null\n",
    "--synonyms          : m [synonym]\n",
    "--template          : d      null\n",
    "--termcol           : d      null\n",
    "--termfile          : d      null\n",
    "--terms             : d      null\n",
    "--transformName     : m {wikidataID=EXTRACT(wikidataURL,.*/(.*))}\n",
    "--wptype            : d      null\n",
    "--input             : d      null\n",
    "--inputnamelist     : d      null\n",
    "--help              : d     false\n",
    "--version           : d     false\n",
    "--dictionary        : d [country]\n",
    "--directory         : d dictionary\n",
    "\n",
    "Specific values (DictionaryCreationTool)\n",
    "================================\n",
    "--testString        : d      null\n",
    "--wikilinks         : d [Lorg.contentmine.ami.tools.AbstractAMIDictTool$WikiLink;@15dcfae7\n",
    "--datacols          : d      null\n",
    "--hrefcols          : d      null\n",
    "--informat          : m wikisparqlxml\n",
    "--linkcol           : d      null\n",
    "--namecol           : d      null\n",
    "--outformats        : d [Lorg.contentmine.ami.tools.AbstractAMIDictTool$DictionaryFileFormat;@17aad511\n",
    "--query             : d      null\n",
    "--sparqlmap         : m {name=wikidataLabel, term=wikidataLabel, description=wikidataDescription, wikidataURL=wikidata, wikidataID=wikidata, wikipediaPage=wikipedia, wikipediaURL=wikipedia, _p297_country=_iso3166}\n",
    "--sparqlquery       : d      null\n",
    "--synonyms          : m [synonym]\n",
    "--template          : d      null\n",
    "--termcol           : d      null\n",
    "--termfile          : d      null\n",
    "--terms             : d      null\n",
    "--transformName     : m {wikidataID=EXTRACT(wikidataURL,.*/(.*))}\n",
    "--wptype            : d      null\n",
    "--input             : d      null\n",
    "--inputnamelist     : d      null\n",
    "--help              : d     false\n",
    "--version           : d     false\n",
    "--dictionary        : d [country]\n",
    "--directory         : d dictionary\n",
    "dictionaryName: country\n",
    "{wikidataLabel=[name, term], wikidataDescription=[description], _iso3166=[_p297_country], wikipedia=[wikipediaPage, wikipediaURL], wikidata=[wikidataURL, wikidataID]}\n",
    "...\n",
    "\n",
    "....\n",
    "<entry _p297_country=\"DK\" description=\"sovereign unitary state in Northern Europe, the Arctic and the North Atlantic\" name=\"Kingdom of Denmark\" term=\"Kingdom of Denmark\" wikidataURL=\"http://www.wikidata.org/entity/Q756617\" wikipediaPage=\"https://en.wikipedia.org/wiki/Kingdom_of_Denmark\" wikipediaURL=\"https://en.wikipedia.org/wiki/Kingdom_of_Denmark\" wikidataID=\"Q756617\"><synonym>DK</synonym><synonym>Denmark</synonym><synonym>Danish Realm</synonym><synonym>the Realm of Denmark</synonym><synonym>Realm of Denmark</synonym></entry>\n",
    "<entry _p297_country=\"SJ\" description=\"two parts of Norway under separate jurisdictions\" name=\"Svalbard and Jan Mayen\" term=\"Svalbard and Jan Mayen\" wikidataURL=\"http://www.wikidata.org/entity/Q842829\" wikipediaPage=\"https://en.wikipedia.org/wiki/Svalbard_and_Jan_Mayen\" wikipediaURL=\"https://en.wikipedia.org/wiki/Svalbard_and_Jan_Mayen\" wikidataID=\"Q842829\"><synonym>sj</synonym></entry>\n",
    "<entry _p297_country=\"AQ\" description=\"area around the South Pole covered by the Antarctic Treaty\" name=\"Antarctic Treaty area\" term=\"Antarctic Treaty area\" wikidataURL=\"http://www.wikidata.org/entity/Q21590062\" wikidataID=\"Q21590062\"><synonym>ATA</synonym><synonym>AQ</synonym><synonym>Antarctica treaty area</synonym></entry>\n",
    "writing dictionary to C:\\Users\\eless\\Documents\\viral_epidemic_country\\dictionary\\country.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is then validated using amidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!amidict --dictionary country --directory dictionary display --validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The results should look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Generic values (DictionaryDisplayTool)\n",
    "================================\n",
    "-v to see generic values\n",
    "\n",
    "Specific values (DictionaryDisplayTool)\n",
    "================================\n",
    "--testString        : d      null\n",
    "--wikilinks         : d [Lorg.contentmine.ami.tools.AbstractAMIDictTool$WikiLink;@2da59753\n",
    "--fields            : d        []\n",
    "--files             : d        []\n",
    "--maxEntries        : d         3\n",
    "--remote            : d [https://github.com/petermr/dictionary]\n",
    "--suffix            : d       xml\n",
    "--validate          : m      true\n",
    "--help              : d     false\n",
    "--version           : d     false\n",
    "--dictionary        : d [country]\n",
    "--directory         : d dictionary\n",
    "    Canada\n",
    "    Netherlands\n",
    "    South Africa\n",
    "    ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
